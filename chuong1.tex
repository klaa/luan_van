\setcounter{chapter}{0}
\setcounter{section}{0}
\begin{center}
\chapter{ Tổng quan học máy và bài toán giảm chiều dữ liệu}
\end{center}

Trong chương này trình bày một số kiến thức tổng quan về học máy và một số khái niệm toán học cần thiết cho việc hiểu và thực hành ứng dụng về giảm chiều dữ liệu sử dụng PCA trong các chương sau. Các kiến thức cơ bản này được tham khảo chủ yếu từ các tài liệu \cite{V1}, \cite{V2}.
%===================================================================
\section{Tổng quan về học máy}
Học máy (Machine Learning) là một lĩnh vực của trí tuệ nhân tạo, nhiệm vụ là xây dựng các hệ thống tự động “học từ dữ liệu” để giải quyết những vấn đề cụ thể. Ứng dụng của học máy trong những năm gần đây xuất hiện ở khắp mọi nơi, từ việc nhận dạng khuôn mặt, vân tay, cử chỉ, tiếng nói đến các hệ thống quản trị rủi ro tài chính, các mô hình xử lí ảnh y khoa,… Học máy có thể ứng dụng hầu hết các loại dữ liệu từ cấu trúc (bảng số liệu kinh tế, số liệu nông nghiệp,…) và cả dữ liệu phi cấu trúc (hình ảnh, văn bản, tín hiệu âm thanh,…). Phần lớn các thử thách mà chúng ta phải đối mặt khi ứng dụng các mô hình học máy vào cuộc sống là việc quản lí dữ liệu và tìm mô hình đúng đắn:
\begin{itemize}
	\item Dữ liệu có thể có kích thước khác nhau, không đầy đủ (bị mất), bị ảnh hưởng của nhiễu, điểm dị biệt và ở nhiều định dạng khác nhau.
	\item Xử lí dữ liệu đòi hỏi một số kỹ năng đặc biệt về loại dữ liệu đó. Ví dụ, như kiểu dữ liệu hình ảnh đòi hỏi cần kiến thức về xử lí ảnh số, dữ liệu âm thanh thì lại có cách xử lí khác.
	\item Tốn thời gian để chọn được một mô hình tốt, việc chọn đúng mô hình nào phù hợp ngay từ ban đầu là một việc khó khăn đầy gian nan. Một mô hình tốt ngay từ đầu nếu không đánh giá kỹ có thể bị quá khớp.
\end{itemize}

Các nội dung dưới đây trình bày các kiến thức tổng quan về học máy.

\subsection{Định nghĩa thuật toán học máy}
Một thuật toán học máy là một thuật toán có khả năng {học tập} từ dữ
liệu. Một chương trình máy tính được gọi là ``học tập'' từ \textit{kinh nghiệm} $E$
để hoàn thành \textit{nhiệm vụ} $T$ với hiệu quả được đo bằng \textit{phép đánh
	giá} $P$, nếu hiệu quả của nó khi thực hiện nhiệm vụ $T$, khi được đánh giá bởi
$P$, cải thiện theo kinh nghiệm $E$.
\subsection{Dữ liệu}
Các \textit{nhiệm vụ} trong học máy được mô tả thông qua việc
một hệ thống xử lý một điểm dữ liệu đầu vào như thế nào.

Một điểm dữ liệu có thể là một bức ảnh, một đoạn âm thanh, một văn bản, hoặc một
tập các hành vi của người dùng trên Internet. Để chương trình máy tính có thể
học được, các điểm dữ liệu thường được đưa về dạng tập hợp các con số mà mỗi số
được gọi là một \textit{đặc trưng} (feature). Vector đặc trưng của một điểm dữ liệu thường được ký hiệu là $\bx \in \R^d$ trong đó $d$ là số lượng đặc trưng. Các mảng nhiều chiều được hiểu là đã bị \textit{vector hoá} (vectorized) thành mảng một chiều.

% Một mô hình học máy được mô tả bởi một bộ các tham số và siêu tham số.
\textit{Kinh nghiệm} trong học máy là bộ dữ liệu được sử dụng để xây
dựng mô hình. Trong quá trình xây dựng mô hình, bộ dữ liệu thường được chia ra
làm ba tập dữ liệu không giao nhau: tập huấn luyện, tập kiểm tra, và tập xác thực.

\textit{Tập huấn luyện} (training set) bao gồm các điểm dữ liệu được sử dụng trực tiếp trong việc xây dựng mô hình. \textit{Tập kiểm tra} (test set) gồm các dữ liệu được dùng để đánh giá hiệu quả của mô hình. Để đảm bảo tính phổ quát, dữ liệu kiểm tra không được sử dụng trong quá trình xây dựng mô hình. Điều kiện cần để một mô hình hiệu quả là kết quả đánh giá trên cả tập huấn luyện và tập kiểm tra đều cao. Tập kiểm tra đại diện cho dữ liệu mà mô hình chưa từng thấy, có thể xuất hiện trong quá trình vận hành mô hình trên thực tế.


\subsection{Các bài toán cơ bản trong học máy}




Nhiều bài toán phức tạp có thể được giải quyết bằng học máy. Dưới đây là một số bài toán phổ biến. \cite{V1}.\\
% \begin{itemize}

\textbf{Phân loại}

\textit{Phân loại} (classification) là một trong những bài toán được nghiên cứu nhiều nhất trong học máy. Trong
bài toán này, chương trình được yêu cầu xác định \textit{lớp/nhãn} (class/label) của một điểm dữ liệu trong số $C$ nhãn khác nhau. Cặp (dữ liệu, nhãn) được ký hiệu là $(\bx, y)$ với $y$ nhận một trong $C$ giá trị trong tập đích $\mathcal{Y}$.
Trong bài toán này, việc xây dựng mô hình tương đương với việc đi tìm hàm số $f$ ánh xạ một điểm dữ liệu $\bx$ vào một phần tử $y \in \mathcal{Y}: y = f(\bx)$. \cite{V1}, \cite{V2}.

\textbf{Hồi quy}

Nếu tập đích $\mathcal{Y}$ gồm các giá trị thực (có thể vô hạn) thì bài toán được gọi là \textit{hồi quy}(regression). Trong bài toán này, ta cần xây dựng một hàm số $f: \R^d \rightarrow \R$.
% \subsection{Transcription}
% Trong loại bài toán này, hệ thống học máy được yêu cầu quan sát một
% loại dữ liệu và



\textbf{Máy dịch}


Trong bài toán \textit{máy dịch} (machine translation), chương trình máy tính được yêu cầu dịch một đoạn văn trong
một ngôn ngữ sang một ngôn ngữ khác. Dữ liệu huấn luyện là các cặp văn bản song
ngữ. Các văn bản này có thể chỉ gồm hai ngôn ngữ đang xét hoặc có thêm các ngôn
ngữ trung gian. Lời giải cho bài toán này gần đây đã có nhiều bước phát triển
vượt bậc dựa trên các thuật toán deep learning.\cite{V2}.


\textbf{Phân cụm}

\textit{Phân cụm} (clustering) là bài toán chia dữ liệu $\mathcal{X}$ thành các cụm nhỏ dựa trên sự liên
quan giữa các dữ liệu trong mỗi cụm. Trong bài toán này, dữ liệu huấn luyện không có nhãn, mô hình tự phân chia dữ liệu thành các cụm khác nhau.\cite{V1}.

\textbf{Hoàn thiện dữ liệu -- data completion}


Một bộ dữ liệu có thể có nhiều đặc trưng nhưng việc thu thập đặc trưng cho từng
điểm dữ liệu đôi khi không khả thi. Chẳng hạn, một bức ảnh có thể bị xước khiến
nhiều điểm ảnh bị mất hay thông tin về tuổi của một số khách hàng không thu thập
được. \textit{Hoàn thiện dữ liệu} (data completion) là bài toán dự đoán các trường dữ liệu còn
thiếu đó. Nhiệm vụ của bài toán này là dựa trên mối tương quan giữa các điểm dữ
liệu để dự đoán những giá trị còn thiếu. Các hệ thống khuyến nghị là một ví dụ điển hình của loại bài toán này. \cite{V1}. Trong đó, hệ thống đưa ra các dự đoán đánh giá của người dùng đối với sản phẩm trên cơ sở các đánh giá trước đó. Từ đó có thể biết người dùng quan tâm, yêu thích sản phẩm nào mà đưa ra những khuyến nghị.


\subsection{Phân nhóm các thuật toán học máy}
Dựa trên tính chất của tập dữ liệu, các thuật toán học máy có thể
được phân thành hai nhóm chính là \textit{học có giám sát} và \textit{học không giám sát}. Ngoài ra, có hai nhóm thuật toán khác gây nhiều chú ý trong thời gian gần đây là \textit{học bán giám sát} và \textit{học củng cố}.\cite{V1}.


\textbf{Học có giám sát}

Một thuật toán học máy được gọi là \textit{học có giám sát} (supervised learning) nếu việc xây dựng mô hình dự đoán mối quan hệ giữa đầu vào và đầu ra được thực hiện dựa trên các cặp (đầu vào, đầu ra) đã biết trong tập huấn luyện. Đây là nhóm thuật toán phổ biến nhất trong các thuật toán học máy.

\textbf{Học không giám sát}


Trong một nhóm các thuật toán khác, dữ liệu huấn luyện chỉ bao gồm các dữ liệu đầu vào $\bx$ mà không có đầu ra tương ứng. Các thuật toán học máy có thể không dự đoán được đầu ra nhưng vẫn trích xuất được những thông tin quan trọng dựa trên mối liên quan giữa các điểm dữ liệu. Các thuật toán trong nhóm này được gọi là \textit{học không giám sát} (unsupervised learning).


\textbf{Học bán giám sát}


Ranh giới giữa học có giám sát và học không giám sát đôi khi không rõ ràng. Có những thuật toán mà tập huấn luyện bao gồm các cặp (đầu vào, đầu ra) và dữ liệu khác chỉ có đầu vào. Những thuật toán này được gọi là \textit{học bán giám sát} (semi-supervised learning).

% Có những bài toán mà dữ liệu được dùng để huấn luyện bao gồm cả những dữ liệu có
% nhãn và chưa được gán nhãn. Các bài toán khi chúng ta có
% một lượng lớn dữ liệu $\mathcal{X}$ nhưng chỉ một phần trong chúng được gán nhãn
% được gọi là \textit{học bán giám sát}, hay \textbf{semi-supervised learning}.
% Những bài toán thuộc nhóm này nằm giữa hai nhóm được nêu bên trên.


\textbf{Học củng cố}

Có một nhóm các thuật toán học máy khác có thể không yêu cầu dữ liệu
huấn luyện mà mô hình học cách ra quyết định bằng cách giao tiếp với môi trường
xung quanh. Các thuật toán thuộc nhóm này liên tục ra quyết định và nhận phản
hồi từ môi trường để tự củng cố hành vi. Nhóm các thuật toán này có tên
\textit{học củng cố} (reinforcement learning).

\subsection{Hàm mất mát và tham số mô hình}

Mỗi mô hình học máy được mô tả bởi bộ \textit{các tham số mô hình} (model parameter).
Công việc của một thuật toán học máy là đi tìm các tham số mô hình tối
ưu cho mỗi bài toán. Việc đi tìm các tham số mô hình có liên quan mật thiết đến
các phép đánh giá. Mục đích chính là đi tìm các tham số mô hình sao cho
các phép đánh giá đạt kết quả cao nhất.

Quan hệ giữa một phép đánh giá và các tham số mô hình được mô tả thông qua một
hàm số gọi là \textit{hàm mất mát} ({loss function} hoặc
{cost function}). Hàm số này thường có giá trị nhỏ khi
phép đánh giá cho kết quả tốt và ngược lại. Việc đi tìm các tham số mô hình sao
cho phép đánh giá trả về kết quả tốt tương đương với việc tối thiểu hàm mất mát.
Như vậy, việc xây dựng một mô hình học máy chính là việc đi giải một
bài toán tối ưu. Quá trình đó được coi là quá trình \textit{learning} của
\textit{machine}. \cite{V1}.

\index{argmin}
Tập hợp các tham số mô hình được ký hiệu bằng $\theta$, hàm mất mát của
mô hình được ký hiệu là $\mathcal{L}(\theta)$ hoặc $J(\theta)$. Bài toán đi tìm tham số mô hình tương đương với bài toán tối thiểu hàm mất mát:
\begin{equation}
\theta^* = \argmin_{\theta}\mathcal{L}(\theta).
\end{equation}
Trong đó, ký hiệu $\displaystyle \argmin_{\theta}\L(\theta)$ được hiểu là giá
trị của $\theta$ để hàm số $\L(\theta)$ đạt giá trị nhỏ nhất. Biến số được ghi
dưới dấu $\argmin$ là biến đang được tối ưu. Biến số này cần được chỉ rõ, trừ
khi hàm mất mát chỉ phụ thuộc vào một biến duy nhất. Ký hiệu $\argmax$ cũng được
sử dụng một cách tương tự khi cần tìm giá trị của các biến số để hàm số đạt giá
trị lớn nhất.

Hàm số $\L(\theta)$ có thể không có chặn dưới hoặc đạt giá trị nhỏ nhất tại
nhiều giá trị $\theta$ khác nhau. Thậm chí, việc tìm giá trị nhỏ nhất của hàm số
này đôi khi không khả thi. Trong các bài toán tối ưu thực tế, việc chỉ cần tìm
ra một bộ tham số $\theta$ khiến hàm mất mát đạt giá trị nhỏ nhất hoặc thậm chí
một giá trị cực tiểu cũng có thể mang lại các kết quả khả quan.
\section{Tổng quan về giảm chiều dữ liệu}
Giảm chiều dữ liệu là một kỹ thuật học máy, là quá trình giảm thiểu số lượng đặc trưng biểu diễn dữ liệu. Việc này có thể được thực hiện theo hướng lựa chọn các đặc trưng quan trọng hoặc trích xuất các đặc trưng mới từ các đặc trưng đã có. Dưới góc độ toán học, giảm chiều dữ liệu là việc đi tìm một hàm số $f: \R^D \rightarrow \R^K$ với $K < D$ biến một điểm dữ liệu $\bx$ trong không gian có số chiều lớn $\R^D$ thành một điểm $\bz$ trong không gian có số chiều nhỏ hơn $\R^D$. 

Việc giảm chiều dữ liệu trước khi thực hiện các thuật toán học máy trên các dữ liệu là rất quan trọng, vì các lý do:
\begin{itemize}
	\item Thứ nhất, dữ liệu thô thường có quá nhiều đặc tính được thu thập. Sẽ có những đặc tính không quan trọng bằng các đặc tính khác. Có những đặc tính lại tương quan với nhau. Quá nhiều đặc tính trong biến thu thập cũng làm cho mô hình thường quá khớp và tập dữ liệu thường thưa.
	\item Thứ hai, cần một không gian lớn để lưu trữ dữ liệu với quá nhiều đặc tính, nhiều chiều.
	\item Thứ ba, việc giảm chiều, giảm số đặc tính mô tả dữ liệu sẽ làm cho việc mô tả và hiểu cũng như trực quan hóa dữ liệu được rõ ràng, dễ hiểu hơn nhiều so với dữ liệu thô ban đầu.
	\item Thứ tư, việc giảm số chiều dữ liệu thường giảm thời gian cần thiết để huấn luyện các mô hình học máy.
	
\end{itemize}
Tuy nhiên, việc giảm chiều dữ liệu cũng giống như việc nén dữ liệu. Giảm chiều nén một tập hợp lớn các đặc tính vào một không gian con mới có số chiều thấp hơn mà không làm mất thông tin quan trọng. Các phương pháp giảm chiều đều làm mất mát một phần thông tin.

Một số phương pháp giảm chiều dữ liệu cơ bản là PCA, LDA và Autoencoder. 

Principle Component Analysis (PCA) là thuật toán giảm chiều dữ liệu được sử dụng rộng rãi nhất.  PCA là một phương pháp thuộc loại unsupervised learning, tức là nó chỉ sử dụng các vector mô tả dữ liệu mà không dùng tới labels, nếu có, của dữ liệu.  PCA là phương pháp giảm chiều dữ liệu sao cho lượng thông tin về dữ liệu, thể hiện ở tổng phương sai, được giữ lại là nhiều nhất. PCA giúp loại bỏ các đặc trưng tương quan, cải thiện hiệu suất thuật toán, giảm quá khớp và cải thiện trực quan hóa dữ liệu. Tuy nhiêu phương pháp này chỉ áp dụng được với các dữ liệu tuyến tính, không giám sát.

Tuy nhiên, trong nhiều trường hợp, ta không cần giữ lại lượng thông tin lớn nhất mà chỉ cần giữ lại thông tin cần thiết cho riêng bài toán đang xét. Bài toán phân lớp là một ví dụ. LDA là một phương pháp giảm chiều dữ liệu cho bài toán phân lớp. LDA có thể được coi là một phương pháp giảm chiều dữ liệu, và cũng có thể được coi là một phương pháp phân lớp, và cũng có thể được áp dụng đồng thời cho cả hai, tức giảm chiều dữ liệu sao cho việc phân lớp hiệu quả nhất. Số chiều của dữ liệu mới là nhỏ hơn hoặc bằng $C-1$, trong đó $C$ là số lượng lớp. Từ ‘Discriminant’ được hiểu là những thông tin đặc trưng cho mỗi lớp, khiến nó không bị lẫn với các lớp khác. Từ ‘Linear’ được dùng vì cách giảm chiều dữ liệu được thực hiện bởi một ma trận chiếu (projection matrix), là một phép biến đổi tuyến tính (linear transform). LDA được dùng cho bài toán học có giám sát, dữ liệu tuyến tính.

Hai phương pháp trên chỉ thực hiện cho dữ liệu tuyến tính. Autoencoder là phương pháp giảm chiều dữ liệu phi tuyến. Bộ tự mã hóa Autoencoder là cách chúng ta sử dụng mạng nơ ron để giảm chiều dữ liệu. Ý tưởng chung của mô hình này là sử dụng một bộ mã hóa (encoder) và bộ giải mã (decoder) để học ra cách biểu diễn dữ liệu tốt nhất. Điều đặc biệt trong kiến trúc của autoencoder là nó tạo ra một nút thắt cổ chai giữa encoder và decoder. Dữ liệu khi đi qua nút thắt cổ chai được mô hình cố gắng khôi phục lại giống với dữ liệu gốc, từ đó các thông tin tại nút thắt là những thông tin đặc trưng tốt nhất cho dữ liệu. 
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{autoendcoder}
	\caption{Kiến trúc của autoendcoder và hàm loss của nó}
	\label{fig:autoendcoder}
\end{figure}
Một số những khác biệt cơ bản giữa Autoendcoder và PCA:
\begin{itemize}
	\item PCA hoạt động tốt trên không gian tuyến tính còn Autoencoder có thể hoạt động trên các hàm phi tuyến phức tạp.
	\item PCA chỉ đơn thuần là việc lấy hình chiếu trực giao do đó các đặc trưng không có nhiều quan hệ. Autoencoder, mặt khác, có các thuộc tính quan hệ với nhau để khôi phục thông tin gốc.
	\item Chi phí tính toán của PCA thấp hơn Autoencoder nhiều lần.
	\item Autoencoder với cấu hình phức tạp có thể bị overfitting.
\end{itemize}

Với mục tiêu áp dụng giảm chiều cho các thuật toán không giám sát với dữ liệu tuyến tính, thuật toán giảm chiều PCA sẽ được tìm hiểu nghiên cứu sâu trong chương 2.

\section{Nền tảng toán học}
Toán học là nền tảng của mọi chuyên ngành nghiên cứu, trong đó có học máy. Đặc biệt các kết quả về đại số tuyến tính, xác suất, thống kê, tối ưu là chìa khóa trong việc xây dựng mô hình và tinh chỉnh kết quả trong học máy. Phần này sẽ hệ thống một số kết quả cần thiết nhất.
\subsection{Cơ sở của một không gian} % (fold)
\label{sub:co_so_cua_mot_khong_gian}

Một hệ các vector $\{\ba_1, \dots, \ba_m\}$ trong không gian vector $m$ chiều $V
= \R^m$ được gọi là một \textit{cơ sở} nếu hai điều kiện sau thoả mãn:
\begin{enumerate}
	\item $V \equiv \text{span}(\ba_1, \dots, \ba_m)$
	\item $\{\ba_1, \dots, \ba_m\}$ là một hệ độc lập tuyến tính.
\end{enumerate}

Khi đó, mọi vector $\bb \in V$ đều có thể biểu diễn \textit{duy nhất} dưới dạng
một tổ hợp tuyến tính của các $\ba_i$.
\subsection{Hệ trực chuẩn, ma trận trực giao} % (fold)


Một hệ cơ sở $\{\mathbf{u}_1, \mathbf{u}_2,\dots, \mathbf{u}_m \in
\mathbb{R}^m\}$ được gọi là \textit{trực giao} nếu mỗi vector khác không và tích
vô hướng của hai vector khác nhau bất kỳ bằng không:
\begin{equation}
\mathbf{u}_i \neq \mathbf{0}; ~~ \mathbf{u}_i^T \mathbf{u}_j = 0
~ \forall ~1 \leq i \neq j \leq m
\end{equation}
Một hệ cơ sở $\{\mathbf{u}_1, \mathbf{u}_2,\dots, \mathbf{u}_m \in
\mathbb{R}^m\}$ được gọi là \textit{trực chuẩn} nếu nó là một hệ \textit{trực
	giao} và độ dài Euclid của mỗi vector bằng 1:
\begin{eqnarray}
\label{eqn:26_4}
\mathbf{u}_i^T \mathbf{u}_j = \left\{
\begin{matrix}
1 & \text{nếu} &i = j \\\
0 & \text{nếu} &i \neq j
\end{matrix}
\right.
\end{eqnarray}
% (o.w. là cách viết ngắn gọn của \textit{trong các trường hợp còn lai} (viết tắt
% của \textit{otherwise}).)
Gọi $\mathbf{U} = [\mathbf{u}_1, \mathbf{u}_2,\dots, \mathbf{u}_m]$ với
$\{\mathbf{u}_1, \mathbf{u}_2,\dots, \mathbf{u}_m \in \mathbb{R}^m\}$ là
\textit{trực chuẩn}. Từ \eqref{eqn:26_4} ta có thể suy ra:
\begin{equation}
\label{eqn:or_matrix}
\mathbf{UU}^T = \mathbf{U}^T\mathbf{U} = \mathbf{I}
\end{equation}
trong đó $\mathbf{I}$ là ma trận đơn vị bậc $m$. Nếu một ma trận thoả mãn điều
kiện~\eqref{eqn:or_matrix}, ta gọi nó là một \textit{ma trận trực giao}.

Một số tính chất \cite{V1}.

\begin{enumerate}
	
	\item \textit{Nghịch đảo của một ma trận trực giao chính là chuyển vị của
		nó.}
	\begin{equation*}
	\mathbf{U}^{-1} = \mathbf{U}^T
	\end{equation*}
	\item \textit{Nếu $\mathbf{U}$ là một ma trận trực giao thì chuyển vị của nó
		$\mathbf{U}^T$ cũng là một ma trận trực giao.}
	\item \textit{Định thức của một ma trận trực giao bằng $1$ hoặc $-1$.}
	
	Điều này có thể suy ra từ việc $\det(\mathbf{U}) = \det(\mathbf{U}^T)$ và
	$\det(\mathbf{U}) \det(\mathbf{U}^T) = \det(\mathbf{I}) = 1$.
	
	\item \textit{Ma trận trực giao thể hiện cho phép xoay một vector}.
	
	Giả sử có hai vector $\mathbf{x,y} \in \mathbb{R}^m$ và một ma trận trực
	giao $\mathbf{U} \in \mathbb{R}^{m \times m}$. Dùng ma trận này để xoay hai
	vector trên ta được $\mathbf{Ux}, \mathbf{Uy}$. Tích vô hướng của hai vector
	mới là:
	\begin{equation*}
	(\mathbf{Ux})^T (\mathbf{Uy}) = \mathbf{x}^T \mathbf{U}^T \mathbf{Uy} = \mathbf{x}^T\mathbf{y}
	\end{equation*}
	như vậy \textit{phép xoay không làm thay đổi tích vô hướng giữa hai vector}.
	
	\item Giả sử $\hat{\mathbf{U}} \in \mathbb{R}^{m \times r}, r < m$ là một ma
	trận con của ma trận trực giao $\mathbf{U}$ được tạo bởi $r$ cột của
	$\mathbf{U}$, ta sẽ có $\hat{\mathbf{U}}^T\hat{\mathbf{U}} =
	\mathbf{I}_{r}$. Việc này có thể được suy ra từ \eqref{eqn:26_4}.
	
\end{enumerate}
\subsection{Biểu diễn véc tơ trong các cơ sở khác nhau}

Trong không gian $m$ chiều, toạ độ của mỗi điểm được xác định dựa trên một hệ
toạ độ nào đó. Ở các hệ toạ độ khác nhau, toạ độ của mỗi điểm cũng khác nhau.

Mỗi vector cột $\mathbf{x} = [x_1, x_2, \dots, x_m] \in \mathbb{R}^m$ có biểu diễn theo hệ cơ sở chính tắc:
\begin{equation}
\mathbf{x} = x_1 \mathbf{e}_1 + x_2 \mathbf{e}_2 + \dots + x_m\mathbf{e}_m
\end{equation}
% \newpage
Giả sử có một hệ cơ sở độc lập tuyến tính khác $\mathbf{u}_1, \mathbf{u}_2,
\dots, \mathbf{u}_m$. Trong hệ cơ sở mới này, $\bx$ được viết dưới dạng
\begin{equation}
\mathbf{x} = y_1 \mathbf{u}_1 + y_2 \mathbf{u}_2 + \dots + y_m\mathbf{u}_m =
\mathbf{U}\mathbf{y}
\end{equation}
với $\mathbf{U} = \bmt \bu_1 & \dots &\bu_m\emt$. Lúc này, vector $\mathbf{y} =
[y_1, y_2, \dots, y_m]^T$ chính là biểu diễn của $\mathbf{x}$ trong hệ cơ sở
mới. Biểu diễn này là duy nhất vì $\mathbf{y} =
\mathbf{U}^{-1} \mathbf{x}$. Đây cũng là biểu thức đổi cơ sở.
Trong các ma trận đóng vai trò như hệ cơ sở, các ma
trận trực giao, tức $\mathbf{U}^T\mathbf{U} = \mathbf{I}$, được quan tâm nhiều
hơn vì nghịch đảo và chuyển vị của chúng bằng nhau, $\mathbf{U}^{-1} = \mathbf{U}^T$.
% \begin{equation}
%     \mathbf{U}^{-1} = \mathbf{U}^T
% \end{equation}
Khi đó, $\mathbf{y}$ có thể được tính một cách nhanh chóng $\mathbf{y} = \mathbf{U}^{T} \mathbf{x}$.
% \begin{equation}
%     \mathbf{y} = \mathbf{U}^{T} \mathbf{x}
% \end{equation}
Từ đó suy ra $y_i = \mathbf{x}^T \mathbf{u}_i = \mathbf{u}_i^T\mathbf{x}, i= 1,
\dots, m$. Việc chuyển đổi hệ cơ sở sử dụng ma trận trực giao có thể được coi như một phép
xoay trục toạ độ. \cite{V1}.
\subsection{Trị riêng và véc tơ riêng}
Cho một ma trận vuông $\bA \in \mathbb{R}^{n\times n}$, một vector $\bx \in \mathbb{C}^n (\bx
\neq \mathbf{0} )$ và một số vô hướng $\lambda \in \mathbb{C}$. Nếu
% $\bA\bx = \lambda \bx$,
\begin{equation}
\bA\bx = \lambda \bx,
\end{equation}
ta nói $\lambda$ là một \textit{trị riêng} của $\bA$, $\bx$ là một \textit{vector riêng} ứng với trị riêng $\lambda$.

Một số tính chất \cite{V1}.
\def\ElA{E_{\lambda}(\bA)}
\begin{enumerate}
	\item Giả sử $\lambda$ là một trị riêng của $\bA \in \mathbb{C}^{n\times n}$, đặt $\ElA$ là tập các vector riêng ứng với trị riêng $\lambda$ đó. Ta có
	\begin{itemize}
		\item Nếu $\bx \in \ElA$ thì $k\bx \in \ElA, \forall k \in \mathbb{C}$.
		
		\item Nếu $\bx_1, \bx_2 \in \ElA$ thì $\bx_1 + \bx_2 \in \ElA$.
	\end{itemize}
	Từ đó suy ra \textit{tập hợp các vector riêng ứng với một trị riêng của một
		ma trận vuông tạo thành một không gian vector con}, thường được gọi là
	\textit{không gian riêng} ứng với trị riêng đó.
	\index{không gian riêng -- eigenspace}
	\index{eigenspace -- không gian riêng}
	
	\item \textit{Mọi ma trận vuông bậc $n$ đều có $n$ trị riêng, kể cả lặp và phức.}
	
	\item \textit{Tích của tất cả các trị riêng của một ma trận bằng định thức
		của ma trận đó. Tổng tất cả các trị riêng của một ma trận bằng tổng các
		phần tử trên đường chéo của ma trận đó.}
	
	
	\item \textit{Phổ của một ma trận bằng phổ của ma trận chuyển vị của nó.}
	
	
	\item \textit{Nếu $\bA, \bB$ là các ma trận vuông cùng bậc thì
		$p_{\bA\bB}(t) = p_ {\bB\bA}(t)$}. Như vậy, tuy $\bA\bB$ có thể khác
	$\bB\bA$, đa thức đặc trưng của $\bA\bB$ và $\bB\bA$ luôn bằng nhau. Tức phổ của hai tích này là trùng nhau.
	
	\item \textit{Tất cả các trị riêng của một ma trận Hermitian là các số
		thực.} 	
	\item Nếu $(\lambda, \bx)$ là một cặp trị riêng, vector riêng của một ma
	trận khả nghịch $\bA$, thì $\displaystyle(\frac{1}{\lambda}, \bx)$ là một
	cặp trị riêng, vector riêng của $\bA^{-1}$, vì $\displaystyle \bA\bx =
	\lambda\bx \Rightarrow \frac{1} {\lambda}\bx = \bA^{-1}\bx$.
	
	
\end{enumerate}
\subsection{Chuẩn của véc tơ và chuẩn của ma trận}
Một hàm số $f: \R^n \rightarrow \R$ được gọi là một chuẩn nếu nó thỏa mãn ba
điều kiện sau đây \cite{1}.
\begin{enumerate}
	
	\item $f(\mathbf{x}) \geq 0$. Dấu bằng xảy ra $\Leftrightarrow \mathbf{x = 0} $.
	
	\item $f(\alpha \mathbf{x}) = |\alpha| f(\mathbf{x}), ~~~\forall \alpha \in \mathbb{R}\ $
	
	\item $f(\mathbf{x}_1) + f(\mathbf{x}_2) \geq f(\mathbf{x}_1 + \mathbf{x}_2),
	~~\forall \mathbf{x}_1, \mathbf{x}_2 \in \mathbb{R}^n$
	
\end{enumerate}
\textbf{Một số chuẩn véc tơ thường dùng}

Độ dài Euclid của một vector $\bx \in \R^n$ chính là một chuẩn, chuẩn này
được gọi là chuẩn $\ell_2$ hoặc chuẩn Euclid:
\begin{equation}
\label{eqn:norm2}
\|\mathbf{x}\|_2 = \sqrt{x_1^2 + x_2^2 + \dots + x_n^2}
\end{equation}
Bình phương của chuẩn $\ell_2$ chính là tích vô hướng của một vector với chính nó,
$\|\mathbf{x}\|_2^2 = \bx^T\bx$.
\index{chuẩn -- norm!chuẩn $\ell_p$}
\index{norm -- chuẩn!chuẩn $\ell_p$}

Với $p$ {là một số không nhỏ hơn 1} bất kỳ, hàm số:
\begin{equation}
\label{eqn:normp}
\|\mathbf{x}\|_p = (|x_1|^p + |x_2|^p + \dots |x_n|^p)^{\frac{1}{p}}
\end{equation}
được chứng minh thỏa mãn ba điều kiện của chuẩn, và được gọi là {chuẩn $\ell_p$}.

Dưới đây là một vài giá trị của $p$ thường được dùng.
\begin{enumerate}
	\item Khi $p = 2$, ta có chuẩn $\ell_2$ như ở trên.
	
	\index{chuẩn -- norm!chuẩn $\ell_1$}
	\index{norm -- chuẩn!chuẩn $\ell_1$}
	\item Khi $p = 1$, ta có chuẩn $\ell_1$:
	$\|\mathbf{x}\|_1 = |x_1| + |x_2| + \dots +|x_n|$ là tổng các trị tuyệt đối
	của từng phần tử của $\mathbf{x}$. 
	
	\item Khi $p \rightarrow \infty $, ta có
	\begin{equation}
	\|\bx\|_{\infty} \triangleq \lim_{p \rightarrow \infty} \|\bx\|_{p} =
	|x_i| = \max_{j=1, 2, \dots, n} | x_j|
	\end{equation}
	
\end{enumerate}
\textbf{Chuẩn Frobenius của ma trận}

Với một ma trận $\mathbf{A} \in \mathbb{R}^{m\times n}$, chuẩn thường được dùng
nhất là chuẩn Frobenius, ký hiệu là $\|\mathbf{A}\|_F$, là căn bậc hai của tổng
bình phương tất cả các phần tử của nó:
\begin{equation*}
\|\mathbf{A}\|_F = \sqrt{\sum_{i = 1}^m \sum_{j = 1}^n a_{ij}^2}
\end{equation*}
\subsection{Vết}
\index{vết -- trace}
\index{trace -- vết}
\textit{Vết} (trace) của một ma trận vuông $\bA$ được ký hiệu là $\trace(\bA)$, là tổng
tất cả các phần tử trên đường chéo chính của nó.

 Một số tính chất của hàm vết
\begin{enumerate}

	\item $\trace(\mathbf{A}) = \sum_{i = 1}^D \lambda_i $ với
	$\mathbf{A}$ là một ma trận vuông và $\lambda_i, i = 1, 2, \dots, N$ là toàn
	bộ các trị riêng của nó, có thể lặp hoặc phức. 

	\item $\|\mathbf{A}\|_F^2 = \trace(\mathbf{A}^T\mathbf{A}) =
	\trace(\mathbf{A}\mathbf{A}^T)$ với $\mathbf{A}$ là một ma trận bất kỳ. Từ
	đây ta cũng suy ra $\trace(\bA\bA^T) \geq 0$ với mọi ma trận $\bA$.
	
\end{enumerate}
\subsection{Kỳ vọng và Phương sai} % (fold)

Cho $N$ giá trị $x_1, x_2, \dots, x_N$. {Kỳ vọng} và {phương sai}
của bộ dữ liệu này được tính theo công thức \cite{V1}.
\begin{eqnarray}
\bar{x} &=& \frac{1}{N}\sum_{n=1}^N x_n = \frac{1}{N}\mathbf{x1},\\\
\sigma^2 &=& \frac{1}{N} \sum_{n=1}^N (x_n - \bar{x})^2,
\end{eqnarray}
với $\bx = \bmt x_1, x_2, \dots, x_N\emt $, và $\mathbf{1} \in \mathbb{R}^N$ là
vector cột chứa toàn phần tử 1. Kỳ vọng đơn giản là trung bình cộng của toàn bộ
các giá trị. Phương sai là trung bình cộng của bình phương khoảng cách từ mỗi
điểm tới kỳ vọng. Phương sai càng nhỏ, các điểm dữ liệu càng gần với kỳ vọng,
tức các điểm dữ liệu càng giống nhau. Phương sai càng lớn, dữ liệu càng có tính
phân tán.

Căn bậc hai của phương sai, $\sigma$ còn được gọi là \textit{độ lệch chuẩn} (standard deviation) của
dữ liệu.
\subsection{Ma trận hiêp phương sai}

Cho $N$ điểm dữ liệu được biểu diễn bởi các vector cột $\mathbf{x}_1, \dots, \mathbf{x}_N$, khi đó, {vector kỳ vọng} và {ma trận hiệp phương sai} của toàn bộ dữ liệu được định nghĩa là \cite{V1}.
\begin{eqnarray}
\bar{\mathbf{x}} &=& \frac{1}{N} \sum_{n=1}^N \mathbf{x}_n, \\\
\mathbf{S} &=&  \frac{1}{N}\sum_{n=1}^N (\mathbf{x}_n - \bar{\mathbf{x}})(\mathbf{x}_n - \bar{\mathbf{x}})^T = \frac{1}{N}\hat{\mathbf{X}}\hat{\mathbf{X}}^T.
\end{eqnarray}
Trong đó $\hat{\mathbf{X}}$ được tạo bằng cách trừ mỗi cột của $\mathbf{X}$ đi $\bar{\mathbf{x}}$:
\begin{equation}
\hat{\mathbf{x}}_n = \mathbf{x}_n - \bar{\mathbf{x}}.
\end{equation}
Một vài tính chất của ma trận hiệp phương sai \cite{V1}.

\begin{enumerate}
	
	\item Ma trận hiệp phương sai là một ma trận đối xứng, hơn nữa, nó là một ma trận nửa xác định dương.
	
	\item Mọi phần tử trên đường chéo của ma trận hiệp phương sai là các số không âm. Chúng chính là phương sai của từng chiều dữ liệu.
	
	\item Các phần tử ngoài đường chéo $s_{ij}, i \neq j$ thể hiện sự tương quan
	giữa thành phần thứ $i$ và thứ $j$ của dữ liệu, còn được gọi là hiệp phương
	sai. Giá trị này có thể dương, âm hoặc bằng không. Khi nó bằng không, ta nói
	rằng hai thành phần $i, j$ trong dữ liệu là \textit{không tương quan}.
	
	\item Nếu ma trận hiệp phương sai là ma trận đường chéo, ta có dữ liệu hoàn toàn không tương quan giữa các chiều.
\end{enumerate}

