\setcounter{chapter}{1}
\setcounter{section}{1}
\chapter{PHƯƠNG PHÁP PCA GIẢM CHIỀU DỮ LIỆU }
%===================================================================
Nội dung chương 2 trình bày phương pháp phân tích thành phần chính (PCA) để giảm chiều dữ liệu. Bản chất của PCA là một phép xoay trục, đổi tọa độ để đạt được mục tiêu. Vì vậy, các kiến thức nền cần được trình bày rõ và hiểu cặn kẽ mới có thể vận dụng tốt trong các ứng dụng. Các kiến thức tham khảo chủ yếu trong \cite{1}, \cite{2},\cite{3}.
\section{Phát biểu bài toán}
Các bài toán thực tế thường có lượng điểm dữ liệu lớn và dữ liệu nhiều chiều. Nếu thực hiện lưu trữ và tính toán trực tiếp trên dữ liệu có số chiều lớn thì sẽ gặp nhiều khó khăn về lưu trữ và xử lý tính toán. Vì vậy, \textit{giảm chiều dữ liệu} là một bước quan trọng trong nhiều bài toán ứng dụng học máy.

Nhìn theo góc độ toán học, giảm chiều dữ liệu sử dụng PCA là việc đi tìm một hàm số $f: \R^D \rightarrow \R^K$ với $K < D$ biến một điểm dữ liệu $\bx$ trong không gian có số chiều lớn $\R^D$ thành một điểm $\bz$ trong không gian có số chiều nhỏ hơn $\R^D$ và chấp nhận một sai số nào đó, đảm bảo
\begin{itemize}
	\item Giảm số chiều của không gian chứa dữ liệu khi nó có số chiều lớn, không thể thể hiện trong không gian 2 hay 3 chiều.
	\item Xây dựng những trục tọa độ mới, thay vì giữ lại các trục của không gian cũ, nhưng lại có khả năng biểu diễn dữ liệu tốt tương đương, và đảm bảo độ biến thiên của dữ liệu trên mỗi chiều mới.
	\item Tạo điều kiện để các liên kết tiềm ẩn của dữ liệu có thể được khám phá trong không gian mới, mà nếu đặt trong không gian cũ thì khó phát hiện vì những liên kết này không thể hiện rõ.
	\item Đảm bảo các trục tọa độ trong không gian mới luôn trực giao đôi một với nhau, mặc dù trong không gian ban đầu các trục có thể không trực giao.
\end{itemize}
\section{Phân tích thành phần chính}
\subsection{Chuyển cơ sở}
Giả sử vector dữ liệu ban đầu $\bx \in \R^{D}$ được giảm chiều trở thành $\bz \in \R^K$ với $K < D$. Một cách đơn giản để giảm chiều dữ liệu từ $D$ về $K < D$ là chỉ giữ lại $K$ phần tử {quan trọng nhất} trong véc tơ dữ liệu. Khi đó, cần trả lời hai câu hỏi quan trọng. Câu hỏi thứ nhất là, làm thế nào để xác định {tầm quan trọng} của mỗi phần tử hay mỗi tọa độ trong véc tơ dữ liệu ban đầu? Từ đó sắp xếp thứ tự tầm quan trọng để giữ lại $K$ thành phần quan trọng nhất. Câu hỏi thứ hai cần trả lời là, nếu tầm quan trọng của các phần tử là như nhau, ta cần xác định bỏ đi những phần tử nào?

Để trả lời câu hỏi thứ nhất, chúng ta sử dụng phương sai của dữ liệu trên mỗi thành phần để làm độ đo sự quan trọng. Nghĩa là theo mỗi thành phần của hệ cơ sở, dữ liệu thể hiện phương sai khác nhau, càng nhỏ thì dữ liệu theo hướng đó càng đồng đều và không mang lại nhiều thông tin. Ngược lại phương sai càng lớn thì độ ngẫu nhiên càng cao và thông tin theo hướng đó càng có giá trị. Ví dụ với dữ liệu trong không gian $2$ chiều, thể hiện trong hình \ref{hinh1}. 

\begin{figure}[ht]
		\includegraphics[width=0.99\linewidth]{Chapters/07_DimemsionalityReduction/27_pca/latex/pca_diagvar.pdf}
		\caption{Ví dụ về các điểm dữ liệu 2 chiều có phương sai theo chiều thứ nhất lớn.}
		\label{hinh1}

\end{figure}

Chúng ta quan sát thấy các điểm dữ liệu có thành phần thứ hai (phương đứng - $e_2$) khá giống nhau hoặc sai khác
nhau không đáng kể (phương sai nhỏ). Khi đó, thành phần này hoàn toàn có thể được lược bỏ, và giá trị của thành phần này sẽ được xấp xỉ bằng kỳ vọng của thành phần đó, nghĩa là bằng trung bình giá trị theo hướng đó, trên toàn bộ dữ liệu. Ngược lại, theo hướng thứ nhất (phương ngang - $e_1$), thì các điểm dữ liệu có thành phần thứ nhất rất khác nhau (phương sai lớn). Do đó nếu xấp xỉ thành phần này bằng kỳ vọng theo như trên, thì {lượng thông tin} bị mất đi đáng kể do sai số xấp xỉ quá lớn.  Vậy là tầm quan trọng của các thành phần có thể đo được bằng lượng thông tin theo mỗi thành phần, nghĩa là có thể được đo bằng phương sai của dữ liệu trên thành phần đó. Tổng lượng thông tin là tổng phương sai trên toàn bộ các thành phần. 

Đối với câu hỏi thứ 2, khi tầm quan trọng hay phương sai của các phần tử là khá tương đương nhau, bỏ đi hướng nào cũng mất mát rất nhiều thông tin vì phương sai lớn. Khi đó chúng ta cần đổi cơ sở sao cho trong hệ cơ sở mới, các điểm dữ liệu có tọa độ thỏa mãn: Theo một số hướng, phương sai đủ nhỏ để bỏ đi. Ví dụ về dữ liệu trong không gian $2$ chiều có cả hai thành phần đều quan trọng dựa theo phương sai được mô tả trong hình \ref{hinh2}.

\begin{figure}[ht]
		\centering
		\includegraphics[width=0.5\linewidth]{Chapters/07_DimemsionalityReduction/27_pca/latex/pca_var0.pdf}
		\label{hinh2}
	\caption{Ví dụ về các điểm dữ liệu 2 chiều có phương sai theo cả 2 chiều đều lớn.}
\end{figure}

\textit{Phân tích thành phần chính} là phương pháp đi tìm một phép xoay trục toạ độ để được một hệ trục toạ độ mới sao
cho trong hệ mới này, thông tin của dữ liệu chủ yếu tập trung ở một vài thành phần. Phần còn lại chứa ít thông tin hơn có thể được lược bỏ. 

Giả sử hệ cơ sở trực chuẩn mới là $\bU$ (mỗi cột của $\bU$ là một vector đơn vị cho một chiều) và ta muốn giữ lại $K$ toạ độ của tất cả các điểm dữ liệu khi biểu diễn trong hệ cơ sở mới này. Không mất tính tổng quát, giả sử đó là $K$ thành phần đầu tiên.
% ******************************************************************************
\begin{figure}[t]
	\centering
	\includegraphics[width = \textwidth]{Chapters/07_DimemsionalityReduction/27_pca/latex/pca_idea.pdf}
	\caption{Mô tả ý tưởng đổi hệ tọa độ của PCA. Dữ liệu được biểu diễn qua hệ cơ sở mới thỏa mãn mong muốn về sự chênh lệch phương sai giữa các thành phần.}
	\label{hinh3}
\end{figure}
% ******************************************************************************
Ý tưởng chính của xây dựng cơ sở mới trong PCA được mô tả trong hình \ref{hinh3}. Theo đó, cơ sở mới $\bU = [\bU_K, \hat{\bU}_K]$ là một hệ trực chuẩn với $\bU_K$ là ma trận con tạo bởi $K$ cột đầu tiên của $\bU$, $\hat{\bU}_K$ là $D-K$ cột còn lại. Trong hệ cơ sở mới này, ma trận dữ liệu được biểu diễn như sau:
\begin{equation}
\label{eqn:27_8}
\bX = \bU_K \mathbf{Z} + \widehat{\bU}_K \mathbf{Y}
\end{equation}
Theo công thức đổi tọa độ khi chuyển cơ sở, và với lưu ý $\bU $ là hệ trực chuẩn, ta có.

\begin{eqnarray}
\label{eqn:27_9}
\left[\begin{matrix} \mathbf{Z} \\\ \mathbf{Y} \end{matrix} \right] = \bU^{-1}\bX = \bU^T \bX= 
\left[\begin{matrix} \bU_K^T \\\ \hat{\bU}_K^T \end{matrix} \right]\bX \Rightarrow
\begin{matrix}
\mathbf{Z} = \bU_K^T \bX \\\
\mathbf{Y} = \hat{\bU}_K^T\bX
\end{matrix}
\end{eqnarray}

Mục đích của PCA là đi tìm ma trận trực giao $\bU$ sao cho phần lớn thông tin
nằm ở $\bU_K \mathbf{Z}$, phần nhỏ thông tin nằm ở $\hat{\bU}_K\mathbf{Y}$.
Phần nhỏ này sẽ được lược bỏ và xấp xỉ bằng một ma trận có các
cột như nhau. Ta hiểu đó là kỳ vọng của dữ liệu theo $D-K$ thành phần. Gọi mỗi cột đó là
$\mathbf{b}$, khi đó, ta mong muốn xấp xỉ $\mathbf{Y} \approx
\mathbf{b1}^T$ với $\mathbf{1}^T\in \mathbb{R}^{1
	\times N}$ là một vector hàng có toàn
bộ $N$ phần tử bằng một. Giả sử đã tìm được $\bU$, ta cần tìm $\mathbf{b}$ thỏa mãn xấp xỉ giữa $\bY$ và $\mathbf{b1}^T$ là tốt nhất, nghĩa là $\mathbf{b}$ là nghiệm của bài toán tối ưu tham số sau:
\begin{equation}
\mathbf{b} = \text{argmin}_{\bb} \|\mathbf{Y} - \mathbf{b1}^T\|_F^2 =
\text{argmin}_{\mathbf{b}} \|\hat{\bU}_K^T\bX - \mathbf{b1}^T\|_F^2
\end{equation}
Với hàm mục tiêu $f(\mathbf{b}) = \|\hat{\bU}_K^T\bX - \mathbf{b1}^T\|_F^2$, ta có đạo hàm $\nabla f(\mathbf{b}) = -{\mathbf{1}^T}^T(\hat{\bU}_K^T\bX - \mathbf{b1}^T)$ = $(\mathbf{b1}^T - \hat{\bU}_K^T\bX)\mathbf{1}$.
Giải phương trình đạo hàm theo $\mathbf{b}$ của hàm mục tiêu bằng $\bzero$:
\begin{equation}
(\mathbf{b1}^T - \hat{\bU}_K^T\bX)\mathbf{1} = 0 \Rightarrow N\mathbf{b} = \hat{\bU}_K^T \mathbf{X1} \Rightarrow \mathbf{b} = \hat{\bU}_K^T \bar{\mathbf{x}}.
\end{equation}
Ở đây ta đã sử dụng $\bone^T\bone = N$ và $\bar{\bx} = \frac{1}{N}\bX\bone$ là
vector trung bình các cột của $\bX$.
Với giá trị $\mathbf{b}$ tìm được này, dữ liệu ban đầu sẽ được xấp xỉ bởi
\begin{equation}
\label{eqn:27_10}
\bX = \bU_K \bZ + \hat{\bU}_k\bY \approx \bU_K\bZ + \hat{\bU}_k \bb\bone^T
= \bU_K \mathbf{Z} + \hat{\bU}_K \hat{\bU}_K^T\bar{\mathbf{x}}\mathbf{1}^T
\triangleq \tilde{\bX}
% \bX \approx \tilde{\bX} = \bU_K \mathbf{Z} + \hat{\bU}_K \hat{\bU}_K^T\bar{\mathbf{x}}\mathbf{1}^T
\end{equation}
\subsection{Hàm mất mát}
Trong cơ sở mới, ma trận dữ liệu $\bX$ đã được xấp xỉ bởi $\tilde{\bX}$.  Thực chất dữ liệu chỉ khác nhau các thành phần được chiếu lên bộ phận hay các hướng $\hat{\bU}_K$, còn các thành phần chính giống nhau. Sai số của phép xấp xỉ này chính là hàm mất mát của phương pháp PCA, là độ lệch trung bình bình phương sau: 
\begin{eqnarray}
\nonumber
\frac{1}{N}\|\bX - \tilde{\bX}\|_F^2 &=&
\frac{1}{N}\|\hat{\bU}_K \bY-  \hat{\bU}_K
\hat{\bU}_K^T\bar{\bx}\bone^T\|_F^2 =
\frac{1}{N}\|\hat{\bU}_K \hat{\bU}_K^T \bX -  \hat{\bU}_K
\hat{\bU}_K^T \bar{\mathbf{x}}\mathbf{1}^T\|_F^2\\
\label{eqn:27_11}
&=& \frac{1}{N} \|\hat{\bU}_k \hat{\bU}_k^T(\bX - \bar{\bx}\bone^T)\|_F^2
\triangleq J(\bU)
\end{eqnarray}
Với các kiến thức chuẩn bị nhắc lại trong chương 1 luận văn ta biết rằng, nếu các cột của một ma trận $\mathbf{V}$ tạo thành một hệ trực chuẩn thì với một ma trận $\mathbf{W}$ bất kỳ thỏa mãn điều kiện nhân, ta luôn có
\begin{equation}
\|\mathbf{VW}\|_F^2 = \text{trace} (\mathbf{W}^T\mathbf{V}^T\mathbf{V} \mathbf{W}) = \text{trace}(\mathbf{W}^T\mathbf{W}) = \|\mathbf{W}\|_F^2
\end{equation}
Đặt $\hat{\bX} = \bX - \bar{\bx}\bone^T$. Ma trận này có được bằng cách trừ
mỗi cột của $\bX$ đi trung bình các cột của nó. Ta gọi $\hat{\bX}$ là {ma trận dữ liệu đã
	được chuẩn hoá}. Ta thấy ngay rằng mỗi cột trong ma trận dữ liệu đã chuẩn hóa là $\hat{\mathbf{x}}_n = \mathbf{x}_n -
\bar{\mathbf{x}},~\forall n = 1, 2, \dots, N$, và ma trận chuẩn hóa này dùng trong công thức tính ma trận hiệp phương sai.

% Ta tạm gọi
% $\hat{\bX}$ là ma trận dữ liệu chuẩn hoá.

Khi đó hàm mất mát trong~\eqref{eqn:27_11} có thể được viết lại thành:
\begin{eqnarray}
J(\bU) &=&  \frac{1}{N} \|\hat{\bU}_K^T\hat{\bX} \|_F^2 = \frac{1}{N}
\|\hat{\bX}^T \hat{\bU}_K \|_F^2 =
\frac{1}{N}\sum_{i = K+1}^D \|\hat{\bX}^T\mathbf{u}_i \|_2^2 \\\
\label{eqn:27_12}
&=& \frac{1}{N} \sum_{i=K+1}^D \mathbf{u}_i^T\hat{\bX}\hat{\bX}^T \mathbf{u}_i
= \sum_{i=K+1}^D \mathbf{u}_i^T\mathbf{S} \mathbf{u}_i
\end{eqnarray}
với $\mathbf{S} = \frac{1}{N}\hat{\bX}\hat{\bX}^T$ là ma trận hiệp phương sai
của dữ liệu và luôn là một ma trận nửa xác định dương (xem lại công thức ma trận hiệp phương sai của dữ liệu ở chương 1).

Tiếp theo ta sẽ giải bài toán tối ưu tham số hàm mất mát, nghĩa là tìm các $\mathbf{u}_i$ để hàm mất mát $J(\bU)$ đạt giá trị nhỏ nhất.

Trước tiên, chúng ta xem xét kết quả sau. Với ma trận $\bU$ trực giao bất kỳ, thay $K = 0$ vào \eqref{eqn:27_12} (lấy tất cả các thành phần của dữ liệu theo hệ tọa độ mới $\bU$), và đặt giá trị đó bằng biến trung gian $L$, ta có
\begin{eqnarray}
L &=& \sum_{i=1}^D \mathbf{u}_i^T\mathbf{Su}_i = \frac{1}{N} \|\hat{\bX}^T\bU\|_F^2 =\frac{1}{N} \text{trace}(\hat{\bX}^T\bU \bU^T \hat{\bX})  \\\
\label{eqn:27_13}
&=& \frac{1}{N} \text{trace} (\hat{\bX}^T \hat{\bX})  = \frac{1}{N} \text{trace} (\hat{\bX} \hat{\bX}^T) =\text{trace} (\mathbf{S}) = \sum_{i=1}^D \lambda_i
\end{eqnarray}
Với $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_D \geq 0$ là các trị riêng của ma trận nửa xác định dương $\mathbf{S}$. Các biến đổi trên có được do các tính chất của chuẩn $F$ cũng như của hàm vết. Chú ý rằng các trị riêng này là thực và không âm, vì tổng các trị riêng của một ma trận vuông bất kỳ luôn bằng vết của ma trận đó.


\textit{Như vậy $L$ không phụ thuộc vào cách chọn ma trận trực giao $\bU$} và bằng tổng các phần tử trên đường chéo của $\mathbf{S}$. Do mỗi thành phần trên đường chéo chính của ma trận hiệp phương sai chính là phương sai của thành phần dữ liệu tương ứng, nên ta có, $L$ chính là tổng các phương sai theo từng thành phần của dữ liệu ban đầu.

Đến đây chúng ta thấy rằng, việc tối thiểu hàm mất mát $J$ được cho bởi \eqref{eqn:27_12} tương
đương với việc tối đa biểu thức
\begin{equation}
F  = L - J = \sum_{i=1}^K \mathbf{u}_i \mathbf{S} \mathbf{u}_i^T
\end{equation}
\subsection{Tối ưu hàm mất mát}
Nghiệm của bài toán tối ưu hàm mất mát PCA được tìm dựa trên khẳng định sau
đây:

	Nếu $\bS$ là một ma trận nửa xác định dương, bài toán tối ưu
	\begin{eqnarray}
	\max_{\bU_K} \sum_{i=1}^K \bu_i^T\bS\bu_i \\
	\text{thoả mãn:}~ \bU_K^T\bU_K = \bI
	\end{eqnarray}
	có nghiệm $\bu_1, \dots, \bu_K$ là các vector riêng ứng với $K$ trị riêng (kể
	cả lặp) lớn
	nhất của $\bS$. Khi đó, giá trị lớn nhất của hàm mục tiêu là $\sum_{i=1}^K\lambda_i$, với
	$\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_D$ là các trị riêng của $\bS$.
Trị riêng lớn nhất $\lambda_1$ của ma trận hiệp phương sai $\bS$ còn được gọi
là \textit{thành
	phần chính thứ nhất}, trị riêng thứ hai $\lambda_2$ được gọi là \textit{thành phần chính thứ hai},... Tên gọi
\textit{phân tích thành phần chính} bắt nguồn từ đây. Ta chỉ giữ lại $K$ thành phần chính đầu tiên khi giảm chiều dữ
liệu dùng PCA. Ma tra trận con $\bU_K$ của hệ trực chuẩn $\bU$ là cột các véc tơ riêng ứng với $K$ giá trị riêng được chọn.

\begin{figure}[ht]
	% caption on side
	\centering
\includegraphics[width=.6\textwidth]{Chapters/07_DimemsionalityReduction/27_pca/latex/pca_var.pdf}
\label{hinh4}
\caption{Minh họa dữ liệu trong hệ cơ sở trực chuẩn tìm được bằng PCA.}
\end{figure}

Hình \ref{hinh4} minh hoạ các thành phần chính với dữ liệu hai chiều.
Trong không gian ban đầu với các vector cơ sở $\mathbf{e}_1,
\mathbf{e}_2$, phương sai theo mỗi chiều dữ liệu (tỉ lệ với độ rộng của các hình chuông
nét liền) đều lớn. Trong hệ cơ sở mới $\mathbf{O}\mathbf{u}_1\mathbf{u}_2$,
phương sai theo chiều thứ hai $\hat{\sigma}_2^2$ nhỏ so với
$\hat{\sigma}_1^2$. Điều này chỉ ra rằng khi chiếu dữ liệu lên $\mathbf{u}_2$, ta
được các điểm rất gần nhau và gần với giá trị trung bình theo chiều đó. Trong
trường hợp này, vì giá trị trung bình theo mọi chiều bằng 0, ta có thể thay thế
toạ độ theo chiều $\mathbf{u}_2$ bằng 0. Rõ ràng là nếu dữ liệu có phương sai
càng nhỏ theo một chiều nào đó thì khi xấp xỉ chiều đó bằng một hằng số, sai số
xấp xỉ càng nhỏ. PCA thực chất là đi tìm một phép xoay tương ứng với một ma trận
trực giao sao cho trong hệ toạ độ mới, tồn tại các chiều có phương sai nhỏ
có thể được bỏ qua; ta chỉ cần giữ lại các chiều/thành phần khác quan trọng hơn. Như
đã khẳng định ở trên, tổng phương sai theo toàn bộ các chiều chiều trong một hệ cơ sở bất kỳ là
như nhau và bằng tổng các trị riêng của ma trận hiệp phương sai. Vì vậy, PCA còn
được coi là phương pháp giảm số chiều dữ liệu sao tổng phương sai còn lại là lớn
nhất.
\subsection{Các bước thực hiện phân tích thành phần chính}
	Từ các suy luận trên, ta có thể tóm tắt lại các bước trong PCA như sau:
	\begin{itemize}
		\item[1)] Tính vector trung bình của toàn bộ dữ liệu:
		\begin{math}
		\bar{\mathbf{x}} = \frac{1}{N} \sum_{n=1}^N \mathbf{x}_n
		\end{math}.
		\item[2)] Trừ mỗi điểm dữ liệu đi vector trung bình của toàn bộ dữ liệu để được dữ
		liệu chuẩn hoá:
		\begin{equation}
		\hat{\mathbf{x}}_n = \mathbf{x}_n - \bar{\mathbf{x}}
		\end{equation}
		\item[3)] Đặt $\hat{\bX} = [\hat{\bx}_1, \hat{\bx}_2, \dots,
		\hat{\bx}_D]$ là ma trận dữ liệu chuẩn hoá, tính ma trận hiệp phương sai:
		\begin{equation}
		\mathbf{S} = \frac{1}{N}\hat{\bX}\hat{\bX}^T
		\end{equation}
		\item[4)] Tính các trị riêng và vector riêng tương ứng có chuẩn bằng 1 của ma trận này, sắp xếp chúng theo thứ tự giảm dần của trị riêng.
		\item[5)] Chọn $K$ vector riêng ứng với $K$ trị riêng lớn nhất để xây dựng ma trận $\bU_K$ có các cột tạo thành một hệ trực giao. $K$ vector này được gọi là các thành phần chính, tạo thành một không gian con {gần} với phân bố của dữ liệu ban đầu đã chuẩn hoá.
		\item[6)] Chiếu dữ liệu ban đầu đã chuẩn hoá $\hat{\bX}$ xuống không gian con tìm được.
		\item[7)] Dữ liệu mới là toạ độ của các điểm dữ liệu trên không gian mới:
		\begin{math}
		\mathbf{Z} = \bU_K^T\hat{\bX}
		\end{math}.
	\end{itemize}
	
	\textit{Như vậy, PCA là kết hợp của phép tịnh tiến, xoay trục toạ độ và chiếu dữ liệu lên hệ toạ độ mới.}
	
	Dữ liệu ban đầu có thể tính được xấp xỉ theo dữ liệu mới bởi
	\begin{math}
	\mathbf{x} \approx \bU_K\mathbf{Z} + \bar{\mathbf{x}}
	\end{math}.
	
	Một điểm dữ liệu mới $\bv \in \R^D$ sẽ
	được giảm chiều bằng PCA theo công thức $\bw = \bU_K^T(\bv - \bar{\bx}) \in
	\R^K$. Ngược lại, nếu biết $\bw$, ta có thể xấp xỉ $\bv$ bởi $\bU_K\bw +
	\bar{\bx}$. Các bước thực hiện PCA được minh hoạ trong Hình \ref{fig:27_5}.
	
	% ******************************************************************************
	\begin{figure}[t]
		\centering
		\includegraphics[width = \textwidth]{Chapters/07_DimemsionalityReduction/27_pca/latex/pca_procedure.pdf}
		\caption{Các bước thực hiện PCA.}
		\label{fig:27_5}
	\end{figure}
	% ******************************************************************************
	 Tóm lại PCA là bài toán đi tìm ma trận
	 trực giao $\bU$ và ma trận mô tả dữ liệu ở không gian thấp chiều $\mathbf{Z}$
	 sao cho việc xấp xỉ sau đây là tốt nhất:
	 \begin{equation}
	 \label{eqn:28_3}
	 \bX \approx \tilde{\bX} = \bU_K \mathbf{Z} + \hat{\bU}_K \hat{\bU}_K^T\bar{\mathbf{x}}\mathbf{1}^T
	 \end{equation}
	 với $\bU_K, \hat{\bU}_K$ lần lượt là các ma trận được tạo bởi $K$ cột đầu tiên
	 và $D-K$ cột cuối cùng của ma trận trực giao $\bU$, và $\bar{\mathbf{x}}$ là
	 vector trung bình của dữ liệu.
	 
	 {Giả sử rằng vector trung bình $\bar{\mathbf{x}} = \mathbf{0}$}. Khi đó, \eqref{eqn:28_3} tương đương với
	 \begin{equation}
	 \label{eqn:28_4}
	 \bX \approx \tilde{\bX} = \bU_K \mathbf{Z}
	 \end{equation}
	 Bài toán tối ưu của PCA sẽ trở thành:
	 \begin{equation}
	 \label{eqn:28_5}
	 \begin{aligned}
	 \bU_K, \mathbf{Z} &=& \arg \min_{\bU_K, \mathbf{Z} } \|\bX - \bU_K
	 \mathbf{Z}\|_F\\\
	 \text{thoả mãn:}&& \bU_K^T \bU_K = \mathbf{I}_K &
	 \end{aligned}
	 \end{equation}
	 với $\mathbf{I}_K \in \mathbb{R}^{K\times K}$ là ma trận đơn vị trong không gian $K$ chiều và điều kiện ràng buộc để đảm bảo các cột của $\bU_K$ tạo thành một hệ trực chuẩn.
	 \subsection{Chọn số chiều của dữ liệu mới}
	 
	 Một câu hỏi được đặt ra là, làm thế nào để chọn giá trị $K$  --  chiều của dữ  liệu mới  --  với từng dữ liệu cụ thể?
	 
	 Thông thường, $K$ được chọn dựa trên việc {quyết định lượng thông tin muốn giữ lại}. Ở đây, toàn bộ thông tin chính là tổng phương sai của toàn bộ các chiều dữ liệu. Lượng dữ liệu muốn dữ lại là tổng phương sai của dữ liệu trong hệ trục toạ độ mới.
	 
	 Chúng ta biết rằng trong mọi hệ trục toạ độ, tổng phương sai của dữ liệu là như nhau 	 và bằng tổng các trị riêng của ma trận hiệp phương sai $\sum_{i=1}^D 	 \lambda_i$. Thêm nữa, PCA giúp giữ lại lượng thông tin (tổng các phương sai) là
	 $\sum_{i=1}^K \lambda_i$. Vậy ta có thể coi biểu thức:
	 \begin{equation}
	 \label{eqn:28_6}
	 r_K = \frac{\sum_{i=1}^K \lambda_i}{\sum_{j=1}^D \lambda_j}
	 \end{equation}
	 là tỉ lệ thông tin được giữ lại khi số chiều dữ liệu mới sau PCA là $K$. Như 	 vậy, chúng ta sẽ chọn K để đảm bảo sai số $r_K$. Chẳng hạn ta muốn giữ lại 95\% dữ liệu, ta chỉ cần chọn $K$ là số tự nhiên 	 nhỏ nhất sao cho $r_K \geq 0.95$. Từ đó quyết định chọn $K$.
	 
	 Khi dữ liệu phân bố quanh một không gian con, các giá trị phương sai lớn nhất 	 ứng với các $\lambda_i$ đầu tiên cao gấp nhiều lần các phương sai còn lại. 	 Khi đó, ta có thể chọn được $K$ khá nhỏ để đạt được $r_K$.
	 
	 
	 \subsection{Một số lưu ý về tính toán PCA}
	 Có hai trường hợp trong thực tế mà chúng ta cần lưu ý về PCA. Trường hợp thứ
	 nhất là lượng dữ liệu có được nhỏ hơn rất nhiều so với số chiều dữ liệu. Trường
	 hợp thứ hai là khi lượng dữ liệu trong tập huấn luyện rất lớn, việc tính toán ma trận hiệp phương sai và trị riêng đôi khi trở nên
	 bất khả thi. Có những hướng giải quyết hiệu quả cho các trường hợp này.
	 
	 {Trong mục này, ta sẽ coi như dữ liệu đã được chuẩn hoá, tức đã được trừ đi
	 	vector kỳ vọng. Khi đó, ma trận hiệp phương sai sẽ là $\mathbf{S} =
	 	\frac{1}{N}\bX\bX^T$.}
	 
	 \textbf{Trường hợp số chiều dữ liệu nhiều hơn số điểm dữ liệu}
	 
	 Đó là trường hợp $D > N$, tức ma trận dữ liệu $\bX$ là một \textit{ma trận cao} (số hàng rất lớn, số cột ít hơn nhiều).
	 Khi đó, số trị riêng khác không của ma trận hiệp phương sai $\mathbf{S}$ sẽ
	 không vượt quá hạng của nó, tức không vượt quá $N$. Vậy ta cần chọn $K \leq N$
	 vì không thể chọn ra được nhiều hơn $N$ trị riêng khác không của một ma trận có hạng
	 bằng $N$.
	 
	 Việc tính toán các trị riêng và vector riêng cũng có thể được thực hiện một cách hiệu quả dựa trên các tính chất sau đây:
	 \begin{enumerate}
	 	\item  Trị riêng của $\mathbf{A}$ cũng là trị riêng của $k\mathbf{A}$ với $k \neq 0$ bất kỳ. Điều này có thể được suy ra trực tiếp từ định nghĩa của trị riêng và vector riêng.
	 	
	 	
	 	\item Trị riêng của $\mathbf{AB}$ cũng là trị riêng của
	 	$\mathbf{BA}$ với $\mathbf{A} \in \mathbb{R}^{d_1 \times d_2}, \mathbf{B} \in \mathbb{R} ^{d_2 \times d_1}$ là các ma trận bất kỳ và $d_1, d_2$ là các số tự nhiên khác không bất kỳ.
	 	
	 	Như vậy, thay vì tìm trị riêng của ma trận hiệp phương sai $\mathbf{S} \in \mathbb{R}^{D\times D}$, ta đi tìm trị riêng của ma trận $\mathbf{T} = \bX^T \bX \in \mathbb{R}^{N \times N}$ có số chiều nhỏ hơn (vì $N < D$).
	 	
	 	\item Nếu $(\lambda, \mathbf{u})$ là một cặp trị riêng,
	 	vector riêng của $\mathbf{T}$ thì $(\lambda, \mathbf{Xu})$ là một cặp trị
	 	riêng, vector riêng của $\mathbf{S}$. Thật vậy:
	 	\begin{eqnarray}
	 	\label{eqn:28_8}
	 	\bX^T \mathbf{Xu} = \bT\bu = \lambda \mathbf{u}
	 	\Rightarrow (\bX\bX^T)(\mathbf{Xu}) = \lambda (\mathbf{Xu} )
	 	\end{eqnarray}
	 	Dấu bằng thứ nhất xảy ra theo định nghĩa của trị riêng và vector riêng.
	 \end{enumerate}
	 
	 Như vậy, ta có thể hoàn toàn tính được trị riêng và vector riêng của ma trận
	 hiệp phương sai $\bS$ dựa trên một ma trận $\bT$ có kích thước nhỏ hơn. Việc
	 này trong nhiều trường hợp khiến thời gian tính toán giảm đi đáng kể.
	 
\textbf{Chuẩn hoá các vector riêng}
	 
Chúng ta biết rằng không gian riêng ứng với trị riêng của một ma trận là không gian sinh (span subspace) tạo bởi toàn bộ các vector riêng ứng với trị riêng đó. Để chuẩn hoá các vector riêng tìm được sao cho chúng tạo thành một hệ trực chuẩn, dựa trên hai điểm sau đây:
	 
	  \textit{Thứ nhất}, nếu $\mathbf{A}$ là một ma trận đối xứng, $(\lambda_1,  \mathbf{x}_1), (\lambda_2, \mathbf{x}_2)$ là các cặp (trị riêng, vector riêng) của $\mathbf{A}$ với $\lambda_1 \neq \lambda_2$, thì  $\mathbf{x}_1^T\mathbf{x}_2 = 0$. Nói cách khác, hai vector bất kỳ trong hai không gian riêng khác nhau của một ma trận đối xứng trực giao với nhau.
	
	 \textit{Thứ hai}, với các trị riêng độc lập tìm được trong một không gian riêng, ta có thể dùng quá trình trực giao hoá Gram-Schmit để chuẩn hoá chúng về một hệ trực chuẩn.
	 
Kết hợp hai điểm trên, ta có thể thu được các vector riêng tạo thành một hệ trực chuẩn, chính là ma trận $\bU_K$ trong PCA.
	 
	 
	 \textbf{Với các bài toán quy mô lớn}
	 
	 Trong rất nhiều bài toán quy mô lớn, ma trận hiệp phương sai là một ma trận rất lớn. Ví dụ, có một triệu bức ảnh
	 1000 $\times$ 1000 pixel, như vậy $D = N = 10^6$ là các số rất lớn, việc trực 	 tiếp tính toán trị riêng và vector riêng cho ma trận hiệp phương sai là không 	 khả thi. Lúc này, các trị riêng và vector riêng của ma trận hiệp phương sai thường được tính thông qua \textit{power method}. Phương pháp này nói rằng, nếu thực hiện quy trình sau, ta sẽ tìm được cặp trị riêng và vector đầu tiên của một ma trận nửa xác định dương:
	 % % <hr>
	 \textit{Phương pháp Power tìm trị riêng và vector riêng của một ma trận nửa xác định dương $\mathbf{A} \in \mathbb{R}^{n \times n}$}:
	  \begin{enumerate}
	   \item Chọn một vector $\mathbf{q}^{(0)} \in \mathbb{R}^n, \|\mathbf{q}^{(0)}\|_2 = 1$ bất kỳ.
	    \item Với $k = 1, 2, \dots$, tính: $\mathbf{z} = \mathbf{Aq}^{(k-1)}$.
	    \item Chuẩn hoá: $\mathbf{q}^{(k)} = \mathbf{z} / \|\mathbf{z}\|_2$.
	    \item Nếu $\|\mathbf{q}^{(k)} - \mathbf{q}^{(k-1)}\|_2$ đủ nhỏ thì dừng lại. Nếu không, $k := k + 1$ rồi quay lại Bước 2.
	   \item $\mathbf{q}^{(k)}$ chính là vector riêng ứng với trị riêng lớn nhất $\lambda_1 = (\mathbf{q}^{(k)})^T\mathbf{A}\mathbf{q}^{(k)}$.
	  \end{enumerate}
	 % % <hr>
	 
	  Quy trình này hội tụ khá nhanh và đã được chứng minh \cite{} \href{http://www.cs.huji.ac.il/~csip/tirgul2.pdf}{tại đây}.
	 
	 Để tìm vector riêng và trị riêng thứ hai của ma trận $\mathbf{A}$, chúng ta dựa trên định lý sau:
	 
	  \textit{Định lý:} Nếu ma trận nửa xác định dương $\mathbf{A}$ có các trị riêng $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_n ( \geq 0)$ và các vector riêng tương ứng $\mathbf{v}_1, \dots, \mathbf{v}_n$, hơn nữa các vector riêng này tạo thành 1 hệ trực chuẩn, thì ma trận:
	  \begin{equation}
	    \mathbf{B} = \mathbf{A} - \lambda_1 \mathbf{v}_1 \mathbf{v}_1^T
	  \end{equation}
	  có các trị riêng $\lambda_2 \geq \lambda_3 \geq \dots \geq \lambda_n \geq 0$ và các vector riêng tương ứng là $\mathbf{v}_2, \mathbf{v}_3, \dots, \mathbf{v}_n, \mathbf{v}_1$.
	 
	  Lúc này, $(\lambda_2, \mathbf{v}_2)$ lại trở thành cặp trị riêng-vector riêng lớn nhất của $\mathbf{B}$. Cách tìm hai biến số này một lần nữa được thực hiện bằng Phương pháp Power.
	 
	 Tiếp tục quy trình này, ta sẽ tìm được (xấp xỉ) tất cả các trị riêng và vector riêng tương ứng của ma trận hiệp phương sai. Ta chỉ cần tìm tới trị riêng thứ $K$ của ma trận hiệp phương sai. 
	 
	 % Phương pháp Power còn là thuật toán cơ bản trong \href{https://en.wikipedia.org/wiki/PageRank}{Google PageRank} giúp sắp xếp các website theo mức độ phổ biến giảm dần. PageRank chính là nền móng của Google; ngày nay, việc tìm kiếm trong Google sử dụng nhiều thuật toán nâng cao hơn PageRank. Tôi sẽ có một bài riêng về Google PageRank sau khi nói về Chuỗi Markov và Mô hình Markov ẩn.
	 
	 
